{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Python Testing with Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes `pytest` Useful?\n",
    "With `pytest`, common tasks require less code and advanced tasks can be achieved through a variety of time-saving commands and plugins. It also runs existing tests out of the box.\n",
    "\n",
    "This tutorial helps to understand some tools `pytest` provides to keep testing efficient and effective even as it scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less Boilerplate\n",
    "Most functional tests follow the Arrange-Act-Assert model:\n",
    "- **Arrange**: setup the conditions for the test\n",
    "- **Act**: call some function or method\n",
    "- **Assert**: assert that some end condition is true\n",
    "\n",
    "Testing frameworks typically hook into your test's assertions so that they can provide information when an assertioin fails. Even a small set of tests requires a fair amount of boilerplate code.\n",
    "\n",
    "```python\n",
    "from unittest import TestCase\n",
    "\n",
    "class TryTesting(TestCase):\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_always_fails(self):\n",
    "        self.assertTrue(False)\n",
    "```\n",
    "\n",
    "These tests can be run from the command line using the `discover` option of `unittest`:\n",
    "\n",
    "`python -m unittest discover`\n",
    "\n",
    "A lot had to be done to run this test:\n",
    "- import `TestCase` from `unittest`\n",
    "- create `TryTesting` subclass\n",
    "- write a method in `TryTesting` for each test\n",
    "- use one of the `self.assert*` methods from `unittest.TestCase` to make assertions\n",
    "\n",
    "This is a significant amount of code to write, and because it's the minimum you need for *any* test, you'd end up writing the same code over and over. `pytest` simplifies this workflow by allowing you to use Python's `assert` keyword directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "    \n",
    "def test_always_fails():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can write an expression that you expect to evaluate to `True`, then pytest will test it for you. It can be run using the `pytest` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_with_pytest.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                   [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_always_fails _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_always_fails\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_with_pytest.py\u001b[0m:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_with_pytest.py::test_always_fails - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.13s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pytest` report shows:\n",
    "- the system state, including which versions of Python, pytest, and any plugins you have installed\n",
    "- the rootdir, the directory to search under for configuration and tests\n",
    "- the number of test the runner discovered\n",
    "\n",
    "The output then indicates the status of each test:\n",
    "- **dot (.)** means the test passed\n",
    "- **F** means the test failed\n",
    "- **E** means the test raised an unexpected exception\n",
    "\n",
    "For tests that fail, the report gives a detailed breakdown of the failure. In the example above, the test failed because `assert False` always fails. Finally, the report gives an overall status report of the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more assertion examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_uppercase():\n",
    "    assert \"loud noises\".upper() == \"LOUD NOISES\"\n",
    "    \n",
    "def test_reversed():\n",
    "    assert list(reversed([1, 2, 3, 4])) == [4, 3, 2, 1]\n",
    "    \n",
    "def test_some_primes():\n",
    "    assert 37 in {\n",
    "        num\n",
    "        \n",
    "        for num in range(1, 50)\n",
    "        if num != 1 and not any(\n",
    "            [num % div == 0 for div in range(2, num)]\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "test_with_pytest.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_always_fails _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_always_fails\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_with_pytest.py\u001b[0m:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_with_pytest.py::test_always_fails - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.15s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve for `pytest` is shallower than for `unittest` because you don't need to learn new constructs for most tests. Also, the use of `assert` makes tests more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Dependency Management\n",
    "Tests should help you make your code more understandable. If the tests themselves are difficult to understand, then you may be in trouble.\n",
    "\n",
    "`pytest` leads you toward **explicit** dependency declarations that are still reusable thanks to the availability of [fixtures](https://docs.pytest.org/en/latest/fixture.html). `pytest` fixtures are functions that create data or test doubles or initialize some system state for the test suite. Any test that wants to use a fixture must explicitly accept it as an argument, so dependencies are always stated up front.\n",
    "\n",
    "Fixtures can also make use of other fixtures, again by declaring them explicitly as dependencies. Over time, your fixtures can become bulky and modular. Although the ability to insert fixtures into other fixtures provides enormous flexibility, it can also make managing dependencies more challenging as your test suite grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Filtering\n",
    "As your test suite grows, you may want to run just a few tests on a feature and save the full suite for later. `pytest` provides a few ways of doings this:\n",
    "- **Name-based filtering**: You can limit pytest to running only those tests whose fully qualified names match a particular expression. You can do this with the `-k` parameter.\n",
    "- **Directory scoping**: By default, `pytest` will run only those tests that are in or under the current directory\n",
    "- **Test categorization**: `pytest` can include or exclude tests from particular categories that you define. You can do this with the `-m` parameter.\n",
    "\n",
    "`pytest` enables you to create **marks**, or custom labels, for any test you like. A test may have multiple labels, and you can use them for granular control over which tests to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameterization\n",
    "When testing functions that process data or perform generic transofrmatoins, you'll be writing many similar tests. They may differ only in input or output of the code being tested. This requires duplicate test code, and doing so can sometimes obscure the behavior you're trying to test.\n",
    "\n",
    "`pytest` offers a solution in which each test can pass or fail independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin-Based Architecture\n",
    "`pytest` has a rich ecosystem of helpful plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures: Managing State and Dependencies\n",
    "`pytest` fixtures are a way of providing data, test doubles, or state setup to your tests. Fixtures are functions that can return a wide range of values. Each test that depends on a fixture must explicitly accept that fixture as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Create Fixtures\n",
    "Imagine you're writing a function, `format_data_for_display()`, to process the data returned by an API endpoint. The data represents a list of people, each with a given name, family name, and job title. The function should output a list of strings that include each person's full name (their `given_name` followed by their `family_name`), a colon, and their `title`. To test this, you might write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_data.py\n",
    "\n",
    "def format_data_for_display(people):\n",
    "    '''\n",
    "    Formats data returned from an API endpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    people : list\n",
    "        A list of people with a given name, family name, and\n",
    "        job title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of strings formatted to include the person's full\n",
    "        name (given name followed by family name) and their title.\n",
    "    '''\n",
    "    return [\n",
    "        f\"{person['given_name']} {person['family_name']}:\"\n",
    "        f\" {person['title']}\"\n",
    "        \n",
    "        for person in people\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_format_data_for_display():\n",
    "    '''\n",
    "    Tests format_data_for_display function.\n",
    "    '''\n",
    "    people = [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    assert format_data_for_display(people) == [\n",
    "        \"Alfonsa Ruiz: Senior Software Engineer\",\n",
    "        \"Sayid Khan: Project Manager\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "format_data.py \u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -s format_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose you need to write another function that transofrms the data into comma-separated values for use in Excel. The test looks familiar to the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_data_csv.py\n",
    "\n",
    "def format_data_for_excel(people):\n",
    "    '''\n",
    "    Formats data returned from an API endpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    people : list\n",
    "        A list of people with a given name, family name, and\n",
    "        job title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string formatted to include the person's full\n",
    "        name (given name followed by family name) and their title\n",
    "        in comma-separated values.\n",
    "    '''\n",
    "    header = ','.join(people[0].keys())\n",
    "    values = [','.join(person.values()) for person in people]\n",
    "    \n",
    "    # rejoin with new lines\n",
    "    values = '\\n'.join(values)\n",
    "    \n",
    "    return f\"{header}\\n{values}\"\n",
    "\n",
    "def test_format_data_for_excel():\n",
    "    people = [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    assert format_data_for_excel(people) == (\n",
    "        \"given_name,family_name,title\\n\"\n",
    "        \"Alfonsa,Ruiz,Senior Software Engineer\\n\"\n",
    "        \"Sayid,Khan,Project Manager\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\r\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "format_data_csv.py \u001b[32m.\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -s format_data_csv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find yourself writing several tests that all make use of underlying test data, then a fixture may be what you want. You can pull repeated data into a single function decorated with `@pytest.fixture` to indicate that the function is a `pytest` fixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_format_fixture.py\n",
    "\n",
    "@pytest.fixture\n",
    "def example_people_data():\n",
    "    return [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this fixture by adding it as an argument to tests. Its value will be the return value of the fixture function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_format_fixture.py\n",
    "\n",
    "def test_format_data_for_display(example_people_data):\n",
    "    assert format_data_for_display(example_people_data) == [\n",
    "        \"Alfonsa Ruiz: Senior Software Engineer\",\n",
    "        \"Sayid Khan: Project Manager\",\n",
    "    ]\n",
    "\n",
    "def test_format_data_for_excel(example_people_data):\n",
    "    assert format_data_for_excel(example_people_data) == (\n",
    "        \"given_name,family_name,title\\n\"\n",
    "        \"Alfonsa,Ruiz,Senior Software Engineer\\n\"\n",
    "        \"Sayid,Khan,Project Manager\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\r\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "data_format_fixture.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -s data_format_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each test is now notably shorter but still has a clear path back to the data it depends on. Be sure to name fixtures something specific. That way it is easily determined if you want to use it when writing new tests in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
