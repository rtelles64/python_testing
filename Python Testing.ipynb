{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Python Testing with Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes `pytest` Useful?\n",
    "With `pytest`, common tasks require less code and advanced tasks can be achieved through a variety of time-saving commands and plugins. It also runs existing tests out of the box.\n",
    "\n",
    "This tutorial helps to understand some tools `pytest` provides to keep testing efficient and effective even as it scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less Boilerplate\n",
    "Most functional tests follow the Arrange-Act-Assert model:\n",
    "- **Arrange**: setup the conditions for the test\n",
    "- **Act**: call some function or method\n",
    "- **Assert**: assert that some end condition is true\n",
    "\n",
    "Testing frameworks typically hook into your test's assertions so that they can provide information when an assertioin fails. Even a small set of tests requires a fair amount of boilerplate code.\n",
    "\n",
    "```python\n",
    "from unittest import TestCase\n",
    "\n",
    "class TryTesting(TestCase):\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_always_fails(self):\n",
    "        self.assertTrue(False)\n",
    "```\n",
    "\n",
    "These tests can be run from the command line using the `discover` option of `unittest`:\n",
    "\n",
    "`python -m unittest discover`\n",
    "\n",
    "A lot had to be done to run this test:\n",
    "- import `TestCase` from `unittest`\n",
    "- create `TryTesting` subclass\n",
    "- write a method in `TryTesting` for each test\n",
    "- use one of the `self.assert*` methods from `unittest.TestCase` to make assertions\n",
    "\n",
    "This is a significant amount of code to write, and because it's the minimum you need for *any* test, you'd end up writing the same code over and over. `pytest` simplifies this workflow by allowing you to use Python's `assert` keyword directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "    \n",
    "def test_always_fails():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can write an expression that you expect to evaluate to `True`, then pytest will test it for you. It can be run using the `pytest` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_with_pytest.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                   [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_always_fails _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_always_fails\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_with_pytest.py\u001b[0m:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_with_pytest.py::test_always_fails - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.13s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pytest` report shows:\n",
    "- the system state, including which versions of Python, pytest, and any plugins you have installed\n",
    "- the rootdir, the directory to search under for configuration and tests\n",
    "- the number of test the runner discovered\n",
    "\n",
    "The output then indicates the status of each test:\n",
    "- **dot (.)** means the test passed\n",
    "- **F** means the test failed\n",
    "- **E** means the test raised an unexpected exception\n",
    "\n",
    "For tests that fail, the report gives a detailed breakdown of the failure. In the example above, the test failed because `assert False` always fails. Finally, the report gives an overall status report of the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more assertion examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_uppercase():\n",
    "    assert \"loud noises\".upper() == \"LOUD NOISES\"\n",
    "    \n",
    "def test_reversed():\n",
    "    assert list(reversed([1, 2, 3, 4])) == [4, 3, 2, 1]\n",
    "    \n",
    "def test_some_primes():\n",
    "    assert 37 in {\n",
    "        num\n",
    "        \n",
    "        for num in range(1, 50)\n",
    "        if num != 1 and not any(\n",
    "            [num % div == 0 for div in range(2, num)]\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "test_with_pytest.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_always_fails _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_always_fails\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_with_pytest.py\u001b[0m:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_with_pytest.py::test_always_fails - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.15s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve for `pytest` is shallower than for `unittest` because you don't need to learn new constructs for most tests. Also, the use of `assert` makes tests more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Dependency Management\n",
    "Tests should help you make your code more understandable. If the tests themselves are difficult to understand, then you may be in trouble.\n",
    "\n",
    "`pytest` leads you toward **explicit** dependency declarations that are still reusable thanks to the availability of [fixtures](https://docs.pytest.org/en/latest/fixture.html). `pytest` fixtures are functions that create data or test doubles or initialize some system state for the test suite. Any test that wants to use a fixture must explicitly accept it as an argument, so dependencies are always stated up front.\n",
    "\n",
    "Fixtures can also make use of other fixtures, again by declaring them explicitly as dependencies. Over time, your fixtures can become bulky and modular. Although the ability to insert fixtures into other fixtures provides enormous flexibility, it can also make managing dependencies more challenging as your test suite grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Filtering\n",
    "As your test suite grows, you may want to run just a few tests on a feature and save the full suite for later. `pytest` provides a few ways of doings this:\n",
    "- **Name-based filtering**: You can limit pytest to running only those tests whose fully qualified names match a particular expression. You can do this with the `-k` parameter.\n",
    "- **Directory scoping**: By default, `pytest` will run only those tests that are in or under the current directory\n",
    "- **Test categorization**: `pytest` can include or exclude tests from particular categories that you define. You can do this with the `-m` parameter.\n",
    "\n",
    "`pytest` enables you to create **marks**, or custom labels, for any test you like. A test may have multiple labels, and you can use them for granular control over which tests to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameterization\n",
    "When testing functions that process data or perform generic transofrmatoins, you'll be writing many similar tests. They may differ only in input or output of the code being tested. This requires duplicate test code, and doing so can sometimes obscure the behavior you're trying to test.\n",
    "\n",
    "`pytest` offers a solution in which each test can pass or fail independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin-Based Architecture\n",
    "`pytest` has a rich ecosystem of helpful plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures: Managing State and Dependencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
