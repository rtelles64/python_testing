{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Python Testing with Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes `pytest` Useful?\n",
    "With `pytest`, common tasks require less code and advanced tasks can be achieved through a variety of time-saving commands and plugins. It also runs existing tests out of the box.\n",
    "\n",
    "This tutorial helps to understand some tools `pytest` provides to keep testing efficient and effective even as it scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less Boilerplate\n",
    "Most functional tests follow the Arrange-Act-Assert model:\n",
    "- **Arrange**: setup the conditions for the test\n",
    "- **Act**: call some function or method\n",
    "- **Assert**: assert that some end condition is true\n",
    "\n",
    "Testing frameworks typically hook into your test's assertions so that they can provide information when an assertioin fails. Even a small set of tests requires a fair amount of boilerplate code.\n",
    "\n",
    "```python\n",
    "from unittest import TestCase\n",
    "\n",
    "class TryTesting(TestCase):\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_always_fails(self):\n",
    "        self.assertTrue(False)\n",
    "```\n",
    "\n",
    "These tests can be run from the command line using the `discover` option of `unittest`:\n",
    "\n",
    "`python -m unittest discover`\n",
    "\n",
    "A lot had to be done to run this test:\n",
    "- import `TestCase` from `unittest`\n",
    "- create `TryTesting` subclass\n",
    "- write a method in `TryTesting` for each test\n",
    "- use one of the `self.assert*` methods from `unittest.TestCase` to make assertions\n",
    "\n",
    "This is a significant amount of code to write, and because it's the minimum you need for *any* test, you'd end up writing the same code over and over. `pytest` simplifies this workflow by allowing you to use Python's `assert` keyword directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "    \n",
    "def test_always_fails():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can write an expression that you expect to evaluate to `True`, then pytest will test it for you. It can be run using the `pytest` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_with_pytest.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                   [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_always_fails _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_always_fails\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_with_pytest.py\u001b[0m:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_with_pytest.py::test_always_fails - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.13s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pytest` report shows:\n",
    "- the system state, including which versions of Python, pytest, and any plugins you have installed\n",
    "- the rootdir, the directory to search under for configuration and tests\n",
    "- the number of test the runner discovered\n",
    "\n",
    "The output then indicates the status of each test:\n",
    "- **dot (.)** means the test passed\n",
    "- **F** means the test failed\n",
    "- **E** means the test raised an unexpected exception\n",
    "\n",
    "For tests that fail, the report gives a detailed breakdown of the failure. In the example above, the test failed because `assert False` always fails. Finally, the report gives an overall status report of the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more assertion examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_uppercase():\n",
    "    assert \"loud noises\".upper() == \"LOUD NOISES\"\n",
    "    \n",
    "def test_reversed():\n",
    "    assert list(reversed([1, 2, 3, 4])) == [4, 3, 2, 1]\n",
    "    \n",
    "def test_some_primes():\n",
    "    assert 37 in {\n",
    "        num\n",
    "        \n",
    "        for num in range(1, 50)\n",
    "        if num != 1 and not any(\n",
    "            [num % div == 0 for div in range(2, num)]\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "test_with_pytest.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_always_fails _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_always_fails\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_with_pytest.py\u001b[0m:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_with_pytest.py::test_always_fails - assert False\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m4 passed\u001b[0m\u001b[31m in 0.15s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve for `pytest` is shallower than for `unittest` because you don't need to learn new constructs for most tests. Also, the use of `assert` makes tests more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Dependency Management\n",
    "Tests should help you make your code more understandable. If the tests themselves are difficult to understand, then you may be in trouble.\n",
    "\n",
    "`pytest` leads you toward **explicit** dependency declarations that are still reusable thanks to the availability of [fixtures](https://docs.pytest.org/en/latest/fixture.html). `pytest` fixtures are functions that create data or test doubles or initialize some system state for the test suite. Any test that wants to use a fixture must explicitly accept it as an argument, so dependencies are always stated up front.\n",
    "\n",
    "Fixtures can also make use of other fixtures, again by declaring them explicitly as dependencies. Over time, your fixtures can become bulky and modular. Although the ability to insert fixtures into other fixtures provides enormous flexibility, it can also make managing dependencies more challenging as your test suite grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Filtering\n",
    "As your test suite grows, you may want to run just a few tests on a feature and save the full suite for later. `pytest` provides a few ways of doings this:\n",
    "- **Name-based filtering**: You can limit pytest to running only those tests whose fully qualified names match a particular expression. You can do this with the `-k` parameter.\n",
    "- **Directory scoping**: By default, `pytest` will run only those tests that are in or under the current directory\n",
    "- **Test categorization**: `pytest` can include or exclude tests from particular categories that you define. You can do this with the `-m` parameter.\n",
    "\n",
    "`pytest` enables you to create **marks**, or custom labels, for any test you like. A test may have multiple labels, and you can use them for granular control over which tests to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parameterization\n",
    "When testing functions that process data or perform generic transofrmatoins, you'll be writing many similar tests. They may differ only in input or output of the code being tested. This requires duplicate test code, and doing so can sometimes obscure the behavior you're trying to test.\n",
    "\n",
    "`pytest` offers a solution in which each test can pass or fail independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugin-Based Architecture\n",
    "`pytest` has a rich ecosystem of helpful plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures: Managing State and Dependencies\n",
    "`pytest` fixtures are a way of providing data, test doubles, or state setup to your tests. Fixtures are functions that can return a wide range of values. Each test that depends on a fixture must explicitly accept that fixture as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Create Fixtures\n",
    "Imagine you're writing a function, `format_data_for_display()`, to process the data returned by an API endpoint. The data represents a list of people, each with a given name, family name, and job title. The function should output a list of strings that include each person's full name (their `given_name` followed by their `family_name`), a colon, and their `title`. To test this, you might write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_data.py\n",
    "\n",
    "def format_data_for_display(people):\n",
    "    '''\n",
    "    Formats data returned from an API endpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    people : list\n",
    "        A list of people with a given name, family name, and\n",
    "        job title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of strings formatted to include the person's full\n",
    "        name (given name followed by family name) and their title.\n",
    "    '''\n",
    "    return [\n",
    "        f\"{person['given_name']} {person['family_name']}:\"\n",
    "        f\" {person['title']}\"\n",
    "        \n",
    "        for person in people\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_format_data_for_display():\n",
    "    '''\n",
    "    Tests format_data_for_display function.\n",
    "    '''\n",
    "    people = [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    assert format_data_for_display(people) == [\n",
    "        \"Alfonsa Ruiz: Senior Software Engineer\",\n",
    "        \"Sayid Khan: Project Manager\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "format_data.py \u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -s format_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose you need to write another function that transofrms the data into comma-separated values for use in Excel. The test looks familiar to the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_data_csv.py\n",
    "\n",
    "def format_data_for_excel(people):\n",
    "    '''\n",
    "    Formats data returned from an API endpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    people : list\n",
    "        A list of people with a given name, family name, and\n",
    "        job title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string formatted to include the person's full\n",
    "        name (given name followed by family name) and their title\n",
    "        in comma-separated values.\n",
    "    '''\n",
    "    header = ','.join(people[0].keys())\n",
    "    values = [','.join(person.values()) for person in people]\n",
    "    \n",
    "    # rejoin with new lines\n",
    "    values = '\\n'.join(values)\n",
    "    \n",
    "    return f\"{header}\\n{values}\"\n",
    "\n",
    "def test_format_data_for_excel():\n",
    "    people = [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    assert format_data_for_excel(people) == (\n",
    "        \"given_name,family_name,title\\n\"\n",
    "        \"Alfonsa,Ruiz,Senior Software Engineer\\n\"\n",
    "        \"Sayid,Khan,Project Manager\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\r\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "format_data_csv.py \u001b[32m.\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -s format_data_csv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find yourself writing several tests that all make use of underlying test data, then a fixture may be what you want. You can pull repeated data into a single function decorated with `@pytest.fixture` to indicate that the function is a `pytest` fixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_format_fixture.py\n",
    "\n",
    "@pytest.fixture\n",
    "def example_people_data():\n",
    "    return [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this fixture by adding it as an argument to tests. Its value will be the return value of the fixture function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_format_fixture.py\n",
    "\n",
    "def test_format_data_for_display(example_people_data):\n",
    "    assert format_data_for_display(example_people_data) == [\n",
    "        \"Alfonsa Ruiz: Senior Software Engineer\",\n",
    "        \"Sayid Khan: Project Manager\",\n",
    "    ]\n",
    "\n",
    "def test_format_data_for_excel(example_people_data):\n",
    "    assert format_data_for_excel(example_people_data) == (\n",
    "        \"given_name,family_name,title\\n\"\n",
    "        \"Alfonsa,Ruiz,Senior Software Engineer\\n\"\n",
    "        \"Sayid,Khan,Project Manager\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.1, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\r\n",
      "rootdir: /Users/roytelles/Desktop/Python Codes/Real Python/Python Testing\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "data_format_fixture.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -s data_format_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each test is now notably shorter but still has a clear path back to the data it depends on. Be sure to name fixtures something specific. That way it is easily determined if you want to use it when writing new tests in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Avoid Fixtures\n",
    "Fixtures aren't great for extracting data or objects that you use across multiple tests. They aren't always as good for tests that require slight variations in the data. Littering your test suite with fixtures is no better than littering it with plain data or objects. It might even be worse because of the added layer of indirection.\n",
    "\n",
    "As with most abstraction, it takes some practice and thought to find the right level of fixture use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures at Scale\n",
    "As you extract more fixtures from your tests, you might see that some fixtures could benefit from further extraction. Fixtures are **modular**, so they can depend on other fixtures. You may find that fixtures in two separate test modules share a common dependency.\n",
    "\n",
    "You can move fixtures from test modules into more general fixture-related modules. That way, you can import them back into any test modules that need them. This is a good approach when you find yourself using a fixture repeatedly throughout your project.\n",
    "\n",
    "pytest looks for `conftest.py` modules throughout the directory structure. Each `conftest.py` provides configuration for the file tree pytest finds in it. You can use any fixtures that are defined in a particular `conftest.py` throughout the file's parent directory and in any subdirectories. This is a great place to put your most widely used fixtures.\n",
    "\n",
    "Another interesting use case for fixtures is in guarding access to resources. Imagine writing a test suite for code that deals with API calls. You want to ensure that the test suite doesn't make any real network calls, even if a test accidentally executes the real network call code. `pytest` provides a [monkeypatch](https://docs.pytest.org/en/latest/monkeypatch.html) fixture to replace values and behaviors, which you can use to great effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conftest.py\n",
    "\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def disable_network_calls(monkeypatch):\n",
    "    def stunted_get():\n",
    "        raise RuntimeError(\n",
    "            \"Network access not allowed during testing!\"\n",
    "        )\n",
    "        \n",
    "    monkeypatch.setattr(\n",
    "        requests,\n",
    "        \"get\",\n",
    "        lambda *args, **kwargs: stunted_get()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By placing `disable_network_calls()` in `conftest.py` and adding the `autouse=True` option, you ensure that network calls will be disabled in every test across the suite. Any test that executes code calling `requests.get()` will raise a `RuntimeError` indicating that an unexpected network call would have occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks: Categorizing Tests\n",
    "In any large test suite, some tests will inevitably be slow. They might test timeout behavior, or they might exercise a broad area of the code. Whatever the reason, it would be nice to avoid running *all* the slow tests when trying to iterate quickly on a new feature.\n",
    "\n",
    "`pytest` enables you to define categories for your tests and provides options for including or excluding categories when you run your suite. You can mark a test with any number of categories.\n",
    "\n",
    "Marking tests is useful for categorizing tests by subsystem or dependencies. If some tests require access to a database, for example, you could create a `@pytest.mark.database_access` mark for them.\n",
    "\n",
    "> **PRO TIP**: Because you can give your marks any name you want, it can be easy to mistype or misremember the name of a mark. pytest will warn you about marks that it doesn't recognize.\n",
    ">\n",
    "> The `--strict-markers` flag to the pytest command ensures that all marks in your tests are registered in your pytest configuration. It will prevent you from running your tests until you register any unknown marks.\n",
    ">\n",
    "> Checkout [registering marks](https://docs.pytest.org/en/latest/mark.html#registering-marks) for more info.\n",
    "\n",
    "When the time comes to run tests, you can still run them all by default with the `pytest` command. If you'd like to run only those tests that require database access, then you can use `pytest -m database_access`. To run all tests *except* those that require database access, you can use `pytest -m \"not database_access\"`. You can even use an `autouse` fixture to limit database access to those tests marked with `database_access`.\n",
    "\n",
    "Some plugins expand on the functionality of marks by guarding access to resources.\n",
    "\n",
    "`pytest` provides a few marks out of the box:\n",
    "- **`skip`**: skips a test unconditionally\n",
    "- **`skipif`**: skips a test if the expression passed to it evaluates to `True`\n",
    "- **`xfail`**: indicates that a test is expected to fail, so if the test *does* fail, the overall suite can still result in a passing status\n",
    "- **`parametrize`**: (note spelling) creates multiple variants of a test with different values as arguments\n",
    "\n",
    "You can see a list of all marks pytest knows by running `pytest --markers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization: Combining Tests\n",
    "Fixtures aren't quite as useful when you have several tests with slightly different inputs and expected outputs. In these cases, you can **parametrize** a single test definitoin, and `pytest` will create variants of the test for you with the parameters you specify.\n",
    "\n",
    "Imagine you've written a function to tell if a string is a palindrome. An initial set of tests could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_is_palindrome_empty_string():\n",
    "    assert is_palindrome(\"\")\n",
    "\n",
    "    \n",
    "def test_is_palindrome_single_character():\n",
    "    assert is_palindrome(\"a\")\n",
    "\n",
    "    \n",
    "def test_is_palindrome_mixed_casing():\n",
    "    assert is_palindrome(\"Bob\")\n",
    "\n",
    "    \n",
    "def test_is_palindrome_with_spaces():\n",
    "    assert is_palindrome(\"Never odd or even\")\n",
    "\n",
    "    \n",
    "def test_is_palindrome_with_punctuation():\n",
    "    assert is_palindrome(\"Do geese see God?\")\n",
    "\n",
    "    \n",
    "def test_is_palindrome_not_palindrome():\n",
    "    assert not is_palindrome(\"abc\")\n",
    "\n",
    "    \n",
    "def test_is_palindrome_not_quite():\n",
    "    assert not is_palindrome(\"abab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these tests excep the last two have the same shape:\n",
    "\n",
    "```python\n",
    "def test_is_palindrome_<in some situation>():\n",
    "    assert is_palindrome(\"<some string>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `@pytest.mark.parametrize()` to fill in this shape with different values, reducing your test code significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"palindrome\", [\n",
    "    \"\",\n",
    "    \"a\",\n",
    "    \"Bob\",\n",
    "    \"Never odd or even\",\n",
    "    \"Do geese see God?\",\n",
    "])\n",
    "def test_is_palindrome(palindrome):\n",
    "    assert is_palindrome(palindrome)\n",
    "    \n",
    "\n",
    "@pytest.mark.parametrize(\"non_palindrome\", [\n",
    "    \"abc\",\n",
    "    \"abab\",\n",
    "])\n",
    "def test_is_palindrome_not_palindrome(non_palindrome):\n",
    "    assert not is_palindrome(non_palindrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument to `parametrize()` is a comma-delimited string of parameter names. The second argument is a list of either tuples or single values that represent the parameter value(s).\n",
    "\n",
    "You can take your parametrization a step further to combine all your tests into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\n",
    "    \"maybe_palindrome, expected_result\",\n",
    "    [\n",
    "        (\"\", True),\n",
    "        (\"a\", True),\n",
    "        (\"Bob\", True),\n",
    "        (\"Never odd or even\", True),\n",
    "        (\"Do geese see God?\", True),\n",
    "        (\"abc\", False),\n",
    "        (\"abab\", False),\n",
    "    ]\n",
    ")\n",
    "def test_is_palindrome(maybe_palindrome, expected_result):\n",
    "    assert is_palindrome(maybe_palindrome) == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: Even though this shortened the code, in this case, it didn't do much to clarify your test code. Use parametrization to separate the test data from the test behavior so that it's clear what the test is testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durations Reports: Fighting Slow Tests\n",
    "Each time you switch contexts from implementation code to test code, you incur some overhead. If your tests are slow to begin with, then overhead can cause friction and frustration.\n",
    "\n",
    "If you want to improve the speed of your tests, then it's useful to know which tests might offer the biggest improvements. `pytest` can automatically record test durations for you and report the top offenders.\n",
    "\n",
    "Use the `--durations` option to the `pytest` command to include a duration report in your test results. `--durations` expects an integer value `n` and will report the slowest n number of tests.\n",
    "\n",
    "The output will follow test results:\n",
    "\n",
    "```shell\n",
    "$ pytest --durations=3\n",
    "3.03s call     test_code.py::test_request_read_timeout\n",
    "1.07s call     test_code.py::test_request_connection_timeout\n",
    "0.57s call     test_code.py::test_database_read\n",
    "======================== 7 passed in 10.06s ==============================\n",
    "```\n",
    "\n",
    "Each test that shows up in the durations report is a good candidate to speed up because it takes an above-average amount of the total testing time.\n",
    "\n",
    "Be aware that some tests may have an invisible setup overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful `pytest` Plugins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pytest-randomly`\n",
    "[`pytest-randomly`](https://github.com/pytest-dev/pytest-randomly) forces your tests to run in a random order. `pytest` always collects all the tests it can find before running them, so `pytest-randomly` shuffles that list of tests just before execution.\n",
    "\n",
    "This is a great way to uncover tests that depend on running in a specific order, which means they have a **stateful dependency** on some other test. If you built your test suite from scratch in pytest, then this isn't very likely. It's more likely to happen in test suites that you migrate to pytest.\n",
    "\n",
    "The plugin will print a seed value in the configuration description. You can use that value to run the tests in the same order as you try to fix the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pytest-cov`\n",
    "If you measure how well your tests cover your implementation code, you likely use the [coverage](https://coverage.readthedocs.io/) package. [`pytest-cov`](https://pytest-cov.readthedocs.io/en/latest/) integrates coverage, so you can run `pytest --cov` to see the test coverage report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pytest-django`\n",
    "[`pytest-django`](https://pytest-django.readthedocs.io/en/latest/) provides a handful of useful fixtures and makrs for dealing with Django tests. The `rf` fixture provides direct access to an instance of Django's [RequestFactory](https://docs.djangoproject.com/en/3.0/topics/testing/advanced/#django.test.RequestFactory). The `setting`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
